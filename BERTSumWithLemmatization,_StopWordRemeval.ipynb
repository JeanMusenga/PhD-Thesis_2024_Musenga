{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPmQE9khgynhtxX+K6qTN8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JeanMusenga/PhD-Thesis_2024_Musenga/blob/main/BERTSumWithLemmatization%2C_StopWordRemeval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://chatgpt.com/share/c64e87ae-aefb-4eef-af28-94d2025276d7\n",
        "\n",
        "To create a more effective extractive summarization approach using the BertSum model, we need to properly integrate the model's capabilities. Here’s a more structured and debugged approach:\n",
        "\n",
        "1.Use the BERT model to encode the text.\n",
        "2.Score each sentence using the BERT model.\n",
        "3.Select the top sentences based on their scores.\n",
        "We will ensure that the sentences are correctly indexed and processed.\n"
      ],
      "metadata": {
        "id": "dLMeV5juPtxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install necessary dependencies"
      ],
      "metadata": {
        "id": "Pc4JziChQ_JH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install nltk\n",
        "!pip install openpyxl"
      ],
      "metadata": {
        "id": "G14fBad4RDU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import necessary libraries"
      ],
      "metadata": {
        "id": "-A4K3rzkRIdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "CbqXWA_TNhZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data"
      ],
      "metadata": {
        "id": "3rMJqFUwQ7Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLkaV3HyNhW-",
        "outputId": "2d0c30f2-ea94-4013-a413-406d7bb6f328"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = './saved_file'\n",
        "file_path = ('DataSampePilot.xlsx')\n",
        "\n",
        "data = pd.read_excel(file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "TqOsGvlTNhUW",
        "outputId": "13ea7f82-76cf-4d2f-fbd3-e150b37aaea3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'DataSampePilot.xlsx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2e9d66750452>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'DataSampePilot.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mshould_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         io = ExcelFile(\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 \u001b[0mext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xls\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m                 ext = inspect_excel_format(\n\u001b[0m\u001b[1;32m   1564\u001b[0m                     \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1565\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1417\u001b[0m         \u001b[0mcontent_or_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_or_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m   1420\u001b[0m         \u001b[0mcontent_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m     ) as handle:\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DataSampePilot.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"Original data:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "Ag08mYnUNhR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nlpyang/BertSum.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4GhTjDzgVGM3",
        "outputId": "b9fee93e-1b9a-4c75-8583-9ee38f0afa37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'BertSum'...\n",
            "remote: Enumerating objects: 301, done.\u001b[K\n",
            "remote: Counting objects: 100% (293/293), done.\u001b[K\n",
            "remote: Compressing objects: 100% (124/124), done.\u001b[K\n",
            "remote: Total 301 (delta 165), reused 290 (delta 164), pack-reused 8\u001b[K\n",
            "Receiving objects: 100% (301/301), 15.05 MiB | 9.76 MiB/s, done.\n",
            "Resolving deltas: 100% (165/165), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List the files in the current directory\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG1RUVk1VLxH",
        "outputId": "a41dbd6d-1d1c-4006-ad74-90c535519a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertSum  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd BertSum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3oH3HUyVPU-",
        "outputId": "b11441dc-b499-4f7a-ebef-5f2201c02acc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/BertSum\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Preprocess the text data"
      ],
      "metadata": {
        "id": "20AbrgsuQ5gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Preprocess the text data\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "j4Z9CKtKNhPW",
        "outputId": "d920a4c9-7b80-421a-b423-0a1fbcf745b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stopwords' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-083a6b6b834e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 3: Preprocess the text data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlemmatizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Tokenize into words\n",
        "    words = word_tokenize(text)\n",
        "    # Remove stopwords and perform lemmatization\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word.lower() not in stop_words]\n",
        "    # Join words back into a single string\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing to the Question_body and Answer_body columns\n",
        "data['Question_body'] = data['Question_body'].apply(lambda x: preprocess_text(x) if pd.notnull(x) else \"\")\n",
        "data['Answer_body'] = data['Answer_body'].apply(lambda x: preprocess_text(x) if pd.notnull(x) else \"\")\n"
      ],
      "metadata": {
        "id": "1QlQBT5BNhNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify preprocessing\n",
        "print(\"Preprocessed data:\")\n",
        "print(data[['Question_body', 'Answer_body']].head())"
      ],
      "metadata": {
        "id": "elRB8GSLNhKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define the BertSum model for extractive summarization"
      ],
      "metadata": {
        "id": "Kqa6UTeHQ3DF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Define the BertSum model for extractive summarization\n",
        "class BertSum:\n",
        "    def __init__(self, model_name='bert-base-uncased'):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        self.model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    def summarize(self, text):\n",
        "        sentences = sent_tokenize(text)\n",
        "        if not sentences:\n",
        "            return \"\"\n",
        "        inputs = self.tokenizer(sentences, return_tensors='pt', padding=True, truncation=True)\n",
        "        outputs = self.model(**inputs)\n",
        "        sentence_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "        scores = torch.norm(sentence_embeddings, dim=1)\n",
        "        top_sentence_idxs = scores.topk(3).indices.tolist()\n",
        "        summary = '. '.join([sentences[idx] for idx in top_sentence_idxs])\n",
        "        return summary"
      ],
      "metadata": {
        "id": "M8VotSUgNhHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "bertsum = BertSum()"
      ],
      "metadata": {
        "id": "P5_H5fdZNhEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Apply the model to the Question_body and Answer_body columns"
      ],
      "metadata": {
        "id": "t4FLzjJsRkV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Apply the model to the Question_body and Answer_body columns\n",
        "def summarize_column(text, column_name):\n",
        "    if pd.notnull(text):\n",
        "        try:\n",
        "            summary = bertsum.summarize(text)\n",
        "            print(f\"Original {column_name}: {text[:100]}...\")  # Print first 100 characters\n",
        "            print(f\"Summary: {summary}\\n\")\n",
        "            return summary\n",
        "        except Exception as e:\n",
        "            print(f\"Error summarizing text: {e}\")\n",
        "            return \"\"\n",
        "    return \"\"\n",
        "\n",
        "data['Question_summary'] = data['Question_body'].apply(lambda x: summarize_column(x, 'Question_body'))\n",
        "data['Answer_summary'] = data['Answer_body'].apply(lambda x: summarize_column(x, 'Answer_body'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-d16mgROAgI",
        "outputId": "30ea6c62-3a57-4ac0-f146-fa5b16ded66e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Question_body: Kinda new AWS . high-level question . Iâ€™m looking insight general architecture workflow , without ...\n",
            "Summary: would React client update message ?. turn , server , code send queue message third party perform action .. waiting response , react app show status update , meaning different stage operation taking place server .\n",
            "\n",
            "Original Question_body: spring boot microservices want use microservices client application ( front-end ) . use Spring MVC d...\n",
            "Summary: spring boot microservices want use microservices client application ( front-end ) .. main logic application resides spring boot microservices .. use Spring MVC designing client side application , , client side application sends request microservices REST APIs use service , standard correct solution ?\n",
            "\n",
            "Original Question_body: 'm trying properly design application according clean architecture , 'm struggling determine layer (...\n",
            "Summary: also possible call UseCase repository .. RemoteData - load cache data API .. version show user depends setting app 's usage mode , example , API return data , user set preference using local data app 's setting .\n",
            "\n",
            "Original Question_body: heard .NET8 Microsoft gifted u totally & quot ; fixed & quot ; authentication authorisation setup . ...\n",
            "Summary: 's trouble another time .. complication even implement identity API , still need return kind identity object Blazor app , since also need know user login OK role .. protected http : // connection Blazor App WebAPI ?\n",
            "\n",
            "Original Question_body: trying learn AWS service , mainly focused API Gateway . understand benefit API gateway listed . Expo...\n",
            "Summary: Expose service REST endpoint Feature like authentication authorization Ability deploy multiple stage Throttling option Still sure whether use API Gateway Infront Kubernetes service deployed EKS .. instance , let consider service microservices spring boot responds http request , contains custom authentication/authorization mechanism using spring security .. Also would like know standard/preferred way create infrastructure AWS service .\n",
            "\n",
            "Original Question_body: process developing Express.js server containerized deployed Azure . server designed store SVG PNG fi...\n",
            "Summary: Given dynamic nature container deployment across various host within Azure , facing challenge accessibility consistency Docker Volumes across different instance host .. process developing Express.js server containerized deployed Azure .. server designed store SVG PNG file , considering use Docker Volumes storage .\n",
            "\n",
            "Original Question_body: 'm trying communicate two device let u assume Device Device B . According documentation online refer...\n",
            "Summary: implement scalable Device Device Communication .. 'm trying communicate two device let u assume Device Device B .. way executing ?\n",
            "\n",
            "Original Question_body: 'm currently developing Android mobile application Android Studio project aimed creating app central...\n",
            "Summary: n't know XML file ( ) arrow defined , n't know either use application make disappear .. searched file present , ca n't find arrow .. three fragment integrated navigation bar .\n",
            "\n",
            "Original Question_body: spring boot web application composed multi-microservices . , possible add spring JWT security micros...\n",
            "Summary: spring boot web application composed multi-microservices .. Thank. , possible add spring JWT security microservice ?\n",
            "\n",
            "Original Question_body: 'm working different architecture setup within MS Fabric layer building process standardize . area g...\n",
            "Summary: Data silver lakehouse ingested notebook processed set analytical component Python R. , seems like limitation driving processed data frame gold warehouse final storage last mile consumption .. Trying determine optimal model would use code notebook push directly data warehouse push data back different silver lakehouse table add another pipeline component go lakehouse warehouse something else ?. concern code side difficult data citizen construct pipeline data flow .\n",
            "\n",
            "Original Answer_body: send request , get response . order send response need keep request reached lambda & quot ; open & q...\n",
            "Summary: polling result ) .. want like ( =client send request , open i.e .. order send response need keep request reached lambda & quot ; open & quot ; ( =lambda running , i.e .\n",
            "\n",
            "Original Answer_body: < blockquote > tl ; dr : Spring MVC contradict trying , technology main scenario built . < /blockquo...\n",
            "Summary: E.g .. < /blockquote >. / use Spring MVC scenario mind ?\n",
            "\n",
            "Original Answer_body: Determining source information business logic , data-access logic . logic belong Repository , belong...\n",
            "Summary: Put interface RemoteUserContactsData LocalUserContactsData push switching UseCase become fairly obvious .. repository might return model , even implement interface , fact need selectable mean business-logic involved belongs UseCase .. logic belong Repository , belongs UseCase .\n",
            "\n",
            "Original Answer_body: always asked question : Microsoft template even documentation offer little want production-ready sys...\n",
            "Summary: tend publish via Azure CDN .. : - ). tabs=app-reg-ga < /a > .\n",
            "\n",
            "Original Answer_body: Short answer : , n't probably . Usually , EKS would deployed private VPC , even make public way , VP...\n",
            "Summary: .. Alternative would implement gateway security Kubernetes cluster could done service mesh like Istio similar .. Usually , EKS would deployed private VPC , even make public way , VPC link API gateway network/application load balancer probably safest way .\n",
            "\n",
            "Original Answer_body: accessing file Docker , use Azure Storage . limitation using Docker Volumes file storage Azure compa...\n",
            "Summary: accessing file Docker , use Azure Storage .. - Click & quot ; Create & quot ; start deployment process .. limitation using Docker Volumes file storage Azure compared Azure Blob Storage performance Scalability .\n",
            "\n",
            "Original Answer_body: device-to-device communication , Azure Event Grid MQTT broker simplest cloud-based solution . Device...\n",
            "Summary: Device publish MQTT topic , Device B subscribe topic .. example : < href= '' http : //learn.microsoft.com/en-us/azure/iot-operations/manage-mqtt-connectivity/overview-iot-mq '' rel= '' nofollow noreferrer '' > http : //learn.microsoft.com/en-us/azure/iot-operations/manage-mqtt-connectivity/overview-iot-mq < /a >. want use IoT Hub , 'll need configure IoT Hub routing send message Device another service Azure Functions generate cloud-to-device message send Device B .\n",
            "\n",
            "Error summarizing text: selected index k out of range\n",
            "Original Answer_body: easiest approach integrate Spring Security ( authentication/authorization ) directly gateway service...\n",
            "Summary: Otherwise , 'd need forward request authentication/authorization service ( like interceptor ) sending actual microservice .. easiest approach integrate Spring Security ( authentication/authorization ) directly gateway service .. 've implemented setup multiple project .\n",
            "\n",
            "Original Answer_body: good thing write T-SQL INSERT ... SELECT pulling data lakehouse inserting warehouse , performance re...\n",
            "Summary: [ iso3 ] = reference .. [ dbo ] .. [ dbo ] .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the summarized data"
      ],
      "metadata": {
        "id": "0E5LTItcRoKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the summarized data\n",
        "print(\"Summarized data:\")\n",
        "print(data[['Question_body', 'Question_summary', 'Answer_body', 'Answer_summary']].head())\n"
      ],
      "metadata": {
        "id": "kcUWGk9uOAdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the summarized data to a new Excel file"
      ],
      "metadata": {
        "id": "NunB-FgLRptn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the summarized data to a new Excel file\n",
        "output_path = '/content/SummarizedData.xlsx'\n",
        "data.to_excel(output_path, index=False, engine='openpyxl')"
      ],
      "metadata": {
        "id": "OsQFW70fOAa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verify that the file has been saved correctly"
      ],
      "metadata": {
        "id": "yuutXMljRsHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "saved_data = pd.read_excel(output_path)\n",
        "print(\"Saved summarized data:\")\n",
        "print(saved_data[['Question_summary', 'Answer_summary']].head())"
      ],
      "metadata": {
        "id": "pmUuWX4IOAWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSC6XFgnOATP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wodeyUmrOAQO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}